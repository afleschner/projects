{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EaJ8AGwpM-2"
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cAxyUwDWt9P"
   },
   "source": [
    "### Business context\n",
    "\n",
    "Employee Promotion means the ascension of an employee to higher ranks, this aspect of the job is what drives employees the most. The ultimate reward for dedication and loyalty towards an organization and the HR team plays an important role in handling all these promotion tasks based on ratings and other attributes available.\n",
    "\n",
    "The HR team in JMD company stored data on the promotion cycle last year, which consists of details of all the employees in the company working last year and also if they got promoted or not, but every time this process gets delayed due to so many details available for each employee - it gets difficult to compare and decide.\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "For the upcoming appraisal cycle, the HR team wants to utilize the stored data and leverage machine learning to make a model that will predict if a person is eligible for promotion or not. You, as a data scientist at JMD company, need to come up with the best possible model that will help the HR team to predict if a person is eligible for promotion or not.\n",
    "\n",
    "\n",
    "### Data Description\n",
    "\n",
    "- employee_id: Unique ID for the employee\n",
    "- department: Department of employee\n",
    "- region: Region of employment (unordered)\n",
    "- education: Education Level\n",
    "- gender: Gender of Employee\n",
    "- recruitment_channel: Channel of recruitment for employee\n",
    "- no_ of_ trainings: no of other training completed in the previous year on soft skills, technical skills, etc.\n",
    "- age: Age of Employee\n",
    "- previous_ year_ rating: Employee Rating for the previous year\n",
    "- length_ of_ service: Length of service in years\n",
    "- awards_ won: if awards won during the previous year then 1 else 0\n",
    "- avg_ training_ score: Average score in current training evaluations\n",
    "- is_promoted: (Target) Recommended for promotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_-uuGqH-qTt"
   },
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHE1Wdc8jUCz"
   },
   "outputs": [],
   "source": [
    "# Installing the libraries with the specified version.\n",
    "# uncomment and run the following line if Google Colab is being used\n",
    "# !pip install scikit-learn==1.2.2 seaborn==0.13.1 matplotlib==3.7.1 numpy==1.25.2 pandas==1.5.3 imbalanced-learn==0.10.1 xgboost==2.0.3 -q --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83D17_Wl4jal"
   },
   "outputs": [],
   "source": [
    "# Installing the libraries with the specified version.\n",
    "# uncomment and run the following lines if Jupyter Notebook is being used\n",
    "# !pip install scikit-learn==1.2.2 seaborn==0.13.1 matplotlib==3.7.1 numpy==1.25.2 pandas==1.5.3 imbalanced-learn==0.10.1 xgboost==2.0.3 -q --user\n",
    "# !pip install --upgrade -q threadpoolctl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lANUqKkk0ra"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxhpZv9y-qTw"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJnKoHy14jam"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvpMDcaaMKtI"
   },
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIiCRwqZ54_C"
   },
   "source": [
    "- Observations\n",
    "- Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01hJQ7EfMKtK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-yGG8LNSSMa"
   },
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bGVKmh75ri8"
   },
   "source": [
    "- EDA is an important part of any project involving data.\n",
    "- It is important to investigate and understand the data better before building a model with it.\n",
    "- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.\n",
    "- A thorough analysis of the data, in addition to the questions mentioned below, should be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEyqzdJBb0jU"
   },
   "source": [
    "**Questions**:\n",
    "\n",
    "1. How is the distribution of average training score of  employee ?\n",
    "2. How is the distribution age of  employee ?\n",
    "3. How does the change in length of service (`length_of_service`) vary by the employee's promotion status (`is_promoted`)?\n",
    "4. How does the previous rating(`previous_year_rating`) vary by the employee's promotion status (`is_promoted`)?\n",
    "5. What are the attributes that have a strong correlation with each other?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YyWJgFlKlWM"
   },
   "source": [
    "#### The below functions need to be defined to carry out the Exploratory Data Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIP4bI3Zbp07"
   },
   "outputs": [],
   "source": [
    "# function to plot a boxplot and a histogram along the same scale.\n",
    "\n",
    "\n",
    "def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n",
    "    \"\"\"\n",
    "    Boxplot and histogram combined\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    figsize: size of figure (default (12,7))\n",
    "    kde: whether to the show density curve (default False)\n",
    "    bins: number of bins for histogram (default None)\n",
    "    \"\"\"\n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
    "        nrows=2,  # Number of rows of the subplot grid= 2\n",
    "        sharex=True,  # x-axis will be shared among all subplots\n",
    "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
    "        figsize=figsize,\n",
    "    )  # creating the 2 subplots\n",
    "    sns.boxplot(\n",
    "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
    "    )  # boxplot will be created and a triangle will indicate the mean value of the column\n",
    "    sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n",
    "    ) if bins else sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
    "    )  # For histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
    "    )  # Add mean to the histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
    "    )  # Add median to the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5021de33"
   },
   "outputs": [],
   "source": [
    "# function to create labeled barplots\n",
    "\n",
    "\n",
    "def labeled_barplot(data, feature, perc=False, n=None):\n",
    "    \"\"\"\n",
    "    Barplot with percentage at the top\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    perc: whether to display percentages instead of count (default is False)\n",
    "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
    "    \"\"\"\n",
    "\n",
    "    total = len(data[feature])  # length of the column\n",
    "    count = data[feature].nunique()\n",
    "    if n is None:\n",
    "        plt.figure(figsize=(count + 1, 5))\n",
    "    else:\n",
    "        plt.figure(figsize=(n + 1, 5))\n",
    "\n",
    "    plt.xticks(rotation=90, fontsize=15)\n",
    "    ax = sns.countplot(\n",
    "        data=data,\n",
    "        x=feature,\n",
    "        palette=\"Paired\",\n",
    "        order=data[feature].value_counts().index[:n].sort_values(),\n",
    "    )\n",
    "\n",
    "    for p in ax.patches:\n",
    "        if perc == True:\n",
    "            label = \"{:.1f}%\".format(\n",
    "                100 * p.get_height() / total\n",
    "            )  # percentage of each class of the category\n",
    "        else:\n",
    "            label = p.get_height()  # count of each level of the category\n",
    "\n",
    "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
    "        y = p.get_height()  # height of the plot\n",
    "\n",
    "        ax.annotate(\n",
    "            label,\n",
    "            (x, y),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            size=12,\n",
    "            xytext=(0, 5),\n",
    "            textcoords=\"offset points\",\n",
    "        )  # annotate the percentage\n",
    "\n",
    "    plt.show()  # show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c08fe5b8"
   },
   "outputs": [],
   "source": [
    "# function to plot stacked bar chart\n",
    "\n",
    "def stacked_barplot(data, predictor, target):\n",
    "    \"\"\"\n",
    "    Print the category counts and plot a stacked bar chart\n",
    "\n",
    "    data: dataframe\n",
    "    predictor: independent variable\n",
    "    target: target variable\n",
    "    \"\"\"\n",
    "    count = data[predictor].nunique()\n",
    "    sorter = data[target].value_counts().index[-1]\n",
    "    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n",
    "        by=sorter, ascending=False\n",
    "    )\n",
    "    print(tab1)\n",
    "    print(\"-\" * 120)\n",
    "    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n",
    "        by=sorter, ascending=False\n",
    "    )\n",
    "    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 1, 5))\n",
    "    plt.legend(\n",
    "        loc=\"lower left\", frameon=False,\n",
    "    )\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e90985c5"
   },
   "outputs": [],
   "source": [
    "### Function to plot distributions\n",
    "\n",
    "def distribution_plot_wrt_target(data, predictor, target):\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    target_uniq = data[target].unique()\n",
    "\n",
    "    axs[0, 0].set_title(\"Distribution of target for target=\" + str(target_uniq[0]))\n",
    "    sns.histplot(\n",
    "        data=data[data[target] == target_uniq[0]],\n",
    "        x=predictor,\n",
    "        kde=True,\n",
    "        ax=axs[0, 0],\n",
    "        color=\"teal\",\n",
    "    )\n",
    "\n",
    "    axs[0, 1].set_title(\"Distribution of target for target=\" + str(target_uniq[1]))\n",
    "    sns.histplot(\n",
    "        data=data[data[target] == target_uniq[1]],\n",
    "        x=predictor,\n",
    "        kde=True,\n",
    "        ax=axs[0, 1],\n",
    "        color=\"orange\",\n",
    "    )\n",
    "\n",
    "    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n",
    "    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n",
    "\n",
    "    axs[1, 1].set_title(\"Boxplot (without outliers) w.r.t target\")\n",
    "    sns.boxplot(\n",
    "        data=data,\n",
    "        x=target,\n",
    "        y=predictor,\n",
    "        ax=axs[1, 1],\n",
    "        showfliers=False,\n",
    "        palette=\"gist_rainbow\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knk0w9XH4jao"
   },
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JbJc1bX4jao"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzOa9FGA6WtG"
   },
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZqmoqz7bp0-"
   },
   "source": [
    "### Model evaluation criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2ORUgmUjDZC"
   },
   "source": [
    "The nature of predictions made by the classification model will translate as follows:\n",
    "\n",
    "- True positives (TP) are failures correctly predicted by the model.\n",
    "- False negatives (FN) are real failures in a generator where there is no detection by model.\n",
    "- False positives (FP) are failure detections in a generator where there is no failure.\n",
    "\n",
    "**Which metric to optimize?**\n",
    "\n",
    "* We need to choose the metric which will ensure that the maximum number of generator failures are predicted correctly by the model.\n",
    "* We would want Recall to be maximized as greater the Recall, the higher the chances of minimizing false negatives.\n",
    "* We want to minimize false negatives because if a model predicts that a machine will have no failure when there will be a failure, it will increase the maintenance cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djQTqGKU4jap"
   },
   "source": [
    "**Let's define a function to output different metrics (including recall) on the train and test set and a function to show confusion matrix so that we do not have to use the same code repetitively while evaluating models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIekBxwp4jaq"
   },
   "outputs": [],
   "source": [
    "# defining a function to compute different metrics to check performance of a classification model built using sklearn\n",
    "def model_performance_classification_sklearn(model, predictors, target):\n",
    "    \"\"\"\n",
    "    Function to compute different metrics to check classification model performance\n",
    "\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "\n",
    "    # predicting using the independent variables\n",
    "    pred = model.predict(predictors)\n",
    "\n",
    "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
    "    recall = recall_score(target, pred)  # to compute Recall\n",
    "    precision = precision_score(target, pred)  # to compute Precision\n",
    "    f1 = f1_score(target, pred)  # to compute F1-score\n",
    "\n",
    "    # creating a dataframe of metrics\n",
    "    df_perf = pd.DataFrame(\n",
    "        {\n",
    "            \"Accuracy\": acc,\n",
    "            \"Recall\": recall,\n",
    "            \"Precision\": precision,\n",
    "            \"F1\": f1\n",
    "\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    return df_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqCDCbcw4jas"
   },
   "source": [
    "### Model Building with original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBtuhurlhKyp"
   },
   "source": [
    "Sample code for model building with original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-tpzI7g4jas"
   },
   "outputs": [],
   "source": [
    "models = []  # Empty list to store all the models\n",
    "\n",
    "# Appending models into the list\n",
    "models.append((\"Bagging\", BaggingClassifier(random_state=1)))\n",
    "'_______' ## Complete the code to append remaining 4 models in the list models\n",
    "\n",
    "print(\"\\n\" \"Training Performance:\" \"\\n\")\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    scores = recall_score(y_train, model.predict(X_train))\n",
    "    print(\"{}: {}\".format(name, scores))\n",
    "\n",
    "print(\"\\n\" \"Validation Performance:\" \"\\n\")\n",
    "\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    scores_val = recall_score(y_val, model.predict(X_val))\n",
    "    print(\"{}: {}\".format(name, scores_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBKJaFU24jas"
   },
   "source": [
    "### Model Building with Oversampled data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKxnygkE4jat"
   },
   "outputs": [],
   "source": [
    "# Synthetic Minority Over Sampling Technique\n",
    "sm = SMOTE(sampling_strategy=1, k_neighbors=5, random_state=1)\n",
    "X_train_over, y_train_over = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYDlbnUO4jat"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aimb6bn4jat"
   },
   "source": [
    "### Model Building with Undersampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DhxfTkvu4jat"
   },
   "outputs": [],
   "source": [
    "# Random undersampler for under sampling the data\n",
    "rus = RandomUnderSampler(random_state=1, sampling_strategy=1)\n",
    "X_train_un, y_train_un = rus.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jROP_DVF4jau"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZGY1eL84jau"
   },
   "source": [
    "### HyperparameterTuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxM3jQuK_Pqc"
   },
   "source": [
    "#### Sample Parameter Grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czq7BZ5b4jau"
   },
   "source": [
    "**Hyperparameter tuning can take a long time to run, so to avoid that time complexity - you can use the following grids, wherever required.**\n",
    "\n",
    "- For Gradient Boosting:\n",
    "\n",
    "```\n",
    "param_grid = {\n",
    "    \"init\": [AdaBoostClassifier(random_state=1),DecisionTreeClassifier(random_state=1)],\n",
    "    \"n_estimators\": np.arange(75,150,25),\n",
    "    \"learning_rate\": [0.1, 0.01, 0.2, 0.05, 1],\n",
    "    \"subsample\":[0.5,0.7,1],\n",
    "    \"max_features\":[0.5,0.7,1],\n",
    "}\n",
    "```\n",
    "\n",
    "- For Adaboost:\n",
    "\n",
    "```\n",
    "param_grid = {\n",
    "     \"n_estimators\": np.arange(10, 110, 10),\n",
    "     \"learning_rate\": [0.1, 0.01, 0.2, 0.05, 1],\n",
    "     \"base_estimator\": [\n",
    "         DecisionTreeClassifier(max_depth=1, random_state=1),\n",
    "         DecisionTreeClassifier(max_depth=2, random_state=1),\n",
    "         DecisionTreeClassifier(max_depth=3, random_state=1),\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "- For Bagging Classifier:\n",
    "\n",
    "```\n",
    "param_grid = {\n",
    "    'max_samples': [0.8,0.9,1],\n",
    "    'max_features': [0.7,0.8,0.9],\n",
    "    'n_estimators' : [30,50,70],\n",
    "}\n",
    "```\n",
    "- For Random Forest:\n",
    "\n",
    "```\n",
    "param_grid = {\n",
    "    \"n_estimators\": [200,250,300],\n",
    "    \"min_samples_leaf\": np.arange(1, 4),\n",
    "    \"max_features\": [np.arange(0.3, 0.6, 0.1),'sqrt'],\n",
    "    \"max_samples\": np.arange(0.4, 0.7, 0.1)\n",
    "}\n",
    "```\n",
    "\n",
    "- For Decision Trees:\n",
    "\n",
    "```\n",
    "param_grid = {\n",
    "    'max_depth': np.arange(2,6),\n",
    "    'min_samples_leaf': [1, 4, 7],\n",
    "    'max_leaf_nodes' : [10, 15],\n",
    "    'min_impurity_decrease': [0.0001,0.001]\n",
    "}\n",
    "```\n",
    "\n",
    "- For XGBoost:\n",
    "\n",
    "```\n",
    "param_grid={\n",
    "   'n_estimators':np.arange(50,300,50),\n",
    "   'scale_pos_weight':[0,1,2,5,10],\n",
    "   'learning_rate':[0.01,0.1,0.2,0.05],\n",
    "   'gamma':[0,1,3,5],\n",
    "   'subsample':[0.7,0.8,0.9,1]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMReRXdH_YUd"
   },
   "source": [
    "#### Sample tuning method for Decision tree with original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9kks1hG_Xhy"
   },
   "outputs": [],
   "source": [
    "# defining model\n",
    "Model = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Parameter grid to pass in RandomSearchCV\n",
    "param_grid = {'max_depth': np.arange(2,6),\n",
    "              'min_samples_leaf': [1, 4, 7],\n",
    "              'max_leaf_nodes' : [10,15],\n",
    "              'min_impurity_decrease': [0.0001,0.001] }\n",
    "\n",
    "#Calling RandomizedSearchCV\n",
    "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
    "\n",
    "#Fitting parameters in RandomizedSearchCV\n",
    "randomized_cv.fit(X_train,y_train)\n",
    "\n",
    "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chN8hbfThKyr"
   },
   "source": [
    "#### Sample tuning method for Decision tree with oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVZcJ0hv4jau"
   },
   "outputs": [],
   "source": [
    "# defining model\n",
    "Model = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Parameter grid to pass in RandomSearchCV\n",
    "param_grid = {'max_depth': np.arange(2,6),\n",
    "              'min_samples_leaf': [1, 4, 7],\n",
    "              'max_leaf_nodes' : [10,15],\n",
    "              'min_impurity_decrease': [0.0001,0.001] }\n",
    "\n",
    "#Calling RandomizedSearchCV\n",
    "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
    "\n",
    "#Fitting parameters in RandomizedSearchCV\n",
    "randomized_cv.fit(X_train_over,y_train_over)\n",
    "\n",
    "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtPIiIS7hKyr"
   },
   "source": [
    "#### Sample tuning method for Decision tree with undersampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pbdykhHhKyr"
   },
   "outputs": [],
   "source": [
    "# defining model\n",
    "Model = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Parameter grid to pass in RandomSearchCV\n",
    "param_grid = {'max_depth': np.arange(2,20),\n",
    "              'min_samples_leaf': [1, 2, 5, 7],\n",
    "              'max_leaf_nodes' : [5, 10,15],\n",
    "              'min_impurity_decrease': [0.0001,0.001] }\n",
    "\n",
    "#Calling RandomizedSearchCV\n",
    "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
    "\n",
    "#Fitting parameters in RandomizedSearchCV\n",
    "randomized_cv.fit(X_train_un,y_train_un)\n",
    "\n",
    "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9JNnpxa4jau"
   },
   "source": [
    "## Model Comparison and Final Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JG85rkY4jav"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_pDMFAz4jav"
   },
   "source": [
    "### Test set final performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8-epsXv4jav"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5hPmHyR4jaw"
   },
   "source": [
    "# Business Insights and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3Ii27CODnbe"
   },
   "source": [
    "-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VB3eO21n_sgt"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e9785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"employee_promotion (2).csv\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"Dataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "df[['education', 'previous_year_rating', 'avg_training_score']] = imputer.fit_transform(df[['education', 'previous_year_rating', 'avg_training_score']])\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "categorical_columns = ['department', 'region', 'education', 'gender', 'recruitment_channel']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Feature selection: Dropping employee_id since it is just an identifier\n",
    "df.drop(columns=['employee_id'], inplace=True)\n",
    "\n",
    "# Split data into features and target\n",
    "X = df.drop(columns=['is_promoted'])\n",
    "y = df['is_promoted']\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X[['no_of_trainings', 'age', 'previous_year_rating', 'length_of_service', 'avg_training_score']] = scaler.fit_transform(X[['no_of_trainings', 'age', 'previous_year_rating', 'length_of_service', 'avg_training_score']])\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Preprocessing complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "v_-uuGqH-qTt"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
